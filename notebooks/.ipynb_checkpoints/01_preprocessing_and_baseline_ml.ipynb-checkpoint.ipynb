{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea017c46-f2b2-44d9-8faa-1461bfaa1c2f",
   "metadata": {},
   "source": [
    "# FarmAI: Data Preprocessing & Baseline Model Development\n",
    "## Feature Extraction using EfficientNetB0 and Classical ML Comparison\n",
    "\n",
    "**Objective:**\n",
    "In this phase of the **FarmAI Analytics** project, we convert raw image data into a format suitable for machine learning. We utilize **Transfer Learning (EfficientNetB0)** to extract high-level features from plant leaves and establish a performance benchmark using classical algorithms (Logistic Regression, Random Forest, SVM). This baseline helps us measure the effectiveness of the deep learning model we will build in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246433d-ac85-404c-b183-37e3994328b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. ENVIRONMENT SETUP & IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a008bb-370e-4088-9816-d463356adf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json # For loading class indices\n",
    "import joblib # For saving baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f9593-e603-4f86-aca2-7523d4c1c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for ML models and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaaae8f-d959-498e-a4b3-b5b6ca03dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow/Keras for image preprocessing and feature extraction\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db3d0f-097b-49b0-93d1-3bbc464d73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path to import src modules\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce7df7-e1b1-4f79-b360-71735ee4a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import config # Import configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c28bdf-25c2-4697-bc6f-e21010f43068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "random.seed(config.RANDOM_SEED)\n",
    "tf.random.set_seed(config.RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f78405-d975-475c-b018-46ef8a78f88d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validate configuration\n",
    "try:\n",
    "    config.validate_config()\n",
    "    config.get_config_summary()\n",
    "    print(\"Environment and configuration loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error validating configuration: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ade564-f07a-49f1-be21-b188cffe5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist\n",
    "config.MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "config.FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "config.METRICS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc36a7f-c5cb-44cc-af0a-19fcd5889862",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Loading & Preparation\n",
    "\n",
    "We start by importing the verified and cleaned dataset generated during the EDA phase of **FarmAI**. This ensures that our models are trained only on high-quality, non-corrupted image data, which is crucial for accurate disease detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7902e-37fa-43f1-b35f-fa3fb1d28204",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1. Load Cleaned Data\n",
    "\n",
    "cleaned_data_path = config.PROCESSED_DATA_DIR / \"eda_cleaned_data.csv\"\n",
    "\n",
    "if not cleaned_data_path.exists():\n",
    "    print(f\"Error: Cleaned data CSV not found at {cleaned_data_path}\")\n",
    "    print(\"Action: Please run '00_data_inspection_and_eda.ipynb' first to generate the cleaned data.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# === LOAD DATA (CRITICAL LINE) ===\n",
    "df = pd.read_csv(cleaned_data_path)\n",
    "print(f\"✓ Data loaded: {len(df)} total images\")\n",
    "print(f\"✓ Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# === SPEED OPTIMIZATION ===\n",
    "print(\"\\n SPEED MODE: Using subset of data for faster testing\")\n",
    "print(f\"Original size: {len(df)} images\")\n",
    "df = df.groupby('label').head(50)  # Now df exists!\n",
    "print(f\"Optimized size: {len(df)} images ({len(df['label'].unique())} classes)\")\n",
    "print(\"  For final model, remove .head(50) line\\n\")\n",
    "\n",
    "# Display sample\n",
    "print(\"Sample of loaded data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a8bed-b03d-4465-abe2-77f5c6ae5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2. Label Encoding & Data Split\n",
    "\n",
    "\"\"\" Labels are encoded to numerical format for machine learning models.\n",
    "Then a deterministic train/validation/test split is created preserving\n",
    "class proportions using stratification.\n",
    "\"\"\" \n",
    "# Encode labels to numerical format for scikit-learn models\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Save class mapping for baseline models\n",
    "class_mapping = {label: int(encoded_label) for label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
    "\n",
    "with open(config.MODELS_DIR / \"baseline_class_indices.json\", 'w') as f:\n",
    "    json.dump(class_mapping, f, indent=4)\n",
    "print(f\"✓ Class mapping for baseline saved to: {config.MODELS_DIR / 'baseline_class_indices.json'}\")\n",
    "\n",
    "# Split data (stratified to maintain class proportions)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    df['filepath'], df['label_encoded'], \n",
    "    test_size=config.TEST_SPLIT, random_state=config.RANDOM_SEED, stratify=df['label_encoded']\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, \n",
    "    test_size=config.VAL_SPLIT / (config.TRAIN_SPLIT + config.VAL_SPLIT), # Adjusted for the remaining split\n",
    "    random_state=config.RANDOM_SEED, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"\\nData split overview:\")\n",
    "print(f\"  Total images: {len(df)}\")\n",
    "print(f\"  Train images: {len(X_train)} ({config.TRAIN_SPLIT*100:.0f}%)\")\n",
    "print(f\"  Validation images: {len(X_val)} ({config.VAL_SPLIT*100:.0f}%)\")\n",
    "print(f\"  Test images: {len(X_test)} ({config.TEST_SPLIT*100:.0f}%)\")\n",
    "print(f\"  Classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Determine if class weights are needed due to imbalance\n",
    "class_counts_train = pd.Series(y_train).value_counts().sort_index()\n",
    "max_count = class_counts_train.max()\n",
    "min_count = class_counts_train.min()\n",
    "if max_count / min_count > 2.0: # If imbalance ratio is significant\n",
    "    print(\"\\nObservation: Class imbalance detected in training data. Class weights will be considered for deep learning models.\")\n",
    "    # For classical ML, sometimes it's handled by algorithm itself or through sampling.\n",
    "else:\n",
    "    print(\"\\nObservation: Training classes appear relatively balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eafbc-5224-4dbf-8816-c3f95f7e88d4",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Extraction (Transfer Learning)\n",
    "\n",
    "To enable **FarmAI** to understand complex leaf patterns without training from scratch, we leverage **EfficientNetB0**, a state-of-the-art Convolutional Neural Network pre-trained on ImageNet.\n",
    "\n",
    "By removing the top classification layer (`include_top=False`), we use EfficientNet solely as a **Feature Extractor**. It converts raw pixels into rich, high-dimensional feature vectors that describe textures, edges, and shapes of the crop diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39734b49-0313-4859-a767-2b5b0be2cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained EfficientNetB0 without its top classification layer\n",
    "# Weights are 'imagenet' as specified in config\n",
    "feature_extractor = EfficientNetB0(weights='imagenet', include_top=False,input_shape=config.INPUT_SHAPE)\n",
    "# Freeze the feature extractor layers to ensure it acts purely as a feature generator\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "def extract_features(filepaths_series, target_size=config.IMG_SIZE, batch_size=config.BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Extracts features from images using the pre-trained EfficientNetB0.\n",
    "    \n",
    "    Args:\n",
    "        filepaths_series (pd.Series): Series of image file paths.\n",
    "        target_size (tuple): Target size for image resizing.\n",
    "        batch_size (int): Batch size for feature extraction.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Flattened features for each image.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting features from {len(filepaths_series)} images...\")\n",
    "    \n",
    "    # Create a TensorFlow Dataset for efficient loading and preprocessing\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filepaths_series)\n",
    "\n",
    "    def load_and_preprocess_image(path):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=config.IMG_CHANNELS)\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img = preprocess_input(img) # EfficientNet specific preprocessing\n",
    "        return img\n",
    "\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    features = []\n",
    "    for batch in dataset:\n",
    "        features.append(feature_extractor(batch, training=False).numpy())\n",
    "    \n",
    "    # Concatenate features and flatten from (batch_size, H, W, C) to (batch_size, H*W*C)\n",
    "    # Flatten features from (batch_size, H, W, C) to (batch_size, H*W*C)\n",
    "    features = np.vstack([f.reshape(f.shape[0], -1) for f in features])\n",
    "    print(f\"✓ Feature extraction complete. Shape: {features.shape}\")\n",
    "    return features\n",
    "\n",
    "# Extract features for train, validation, and test sets\n",
    "X_train_features = extract_features(X_train)\n",
    "X_val_features = extract_features(X_val)\n",
    "X_test_features = extract_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce65ef-5386-422d-b426-de29d8273cb9",
   "metadata": {},
   "source": [
    "## 4. Establishing Baseline Performance\n",
    "\n",
    "Before building a heavy Deep Learning model, we verify the quality of our features by training classical Machine Learning algorithms. If simple models like **Random Forest** or **SVM** perform well on the extracted features, it confirms that **FarmAI's** feature extraction pipeline is robust and ready for the final Deep Learning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc32ef1-035b-4aff-a98f-1c64b713faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1. Logistic Regression\n",
    "\n",
    "## A simple linear model often used as a strong baseline demonstrating basic discriminative power.\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Logistic Regression ---\")\n",
    "log_reg_model = LogisticRegression(max_iter=500, random_state=config.RANDOM_SEED)\n",
    "log_reg_model.fit(X_train_features, y_train)\n",
    "log_reg_predictions = log_reg_model.predict(X_test_features)\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_predictions)\n",
    "\n",
    "print(f\"Accuracy: {log_reg_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, log_reg_predictions, \n",
    "                            target_names=[label_encoder.inverse_transform([i])[0].replace('___', ' ') for i in sorted(y_test.unique())],\n",
    "                            digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceaa5d1-a16d-48b7-9ded-ab1ab4564498",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2. Random Forest Classifier\n",
    "\n",
    "## An ensemble method known for its robustness non-linearity and good performance on varied data.\n",
    "\n",
    "print(\"\\n--- Training Random Forest Classifier ---\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=config.RANDOM_SEED, n_jobs=-1)\n",
    "rf_model.fit(X_train_features, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_features)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions, \n",
    "                            target_names=[label_encoder.inverse_transform([i])[0].replace('___', ' ') for i in sorted(y_test.unique())],\n",
    "                            digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d421de-e40b-4c5d-8848-a58219e086a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.3. Support Vector Machine (SVM)\n",
    "\n",
    "'''\n",
    "A powerful algorithm for classification especially effective in high-dimensional spaces\n",
    "capable of finding complex decision boundaries. We use a linear kernel for computational efficiency.\n",
    "'''\n",
    "\n",
    "print(\"\\n--- Training Support Vector Machine (Linear Kernel) ---\")\n",
    "# For larger datasets, consider using LinearSVC for even faster training\n",
    "svm_model = SVC(kernel='linear', random_state=config.RANDOM_SEED, verbose=False) \n",
    "svm_model.fit(X_train_features, y_train)\n",
    "svm_predictions = svm_model.predict(X_test_features)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(f\"Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions, \n",
    "                            target_names=[label_encoder.inverse_transform([i])[0].replace('___', ' ') for i in sorted(y_test.unique())],\n",
    "                            digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9facd90-2336-4ea4-a80c-2ba2d1c0c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4. Baseline Model Comparison Summary\n",
    "\n",
    "baseline_results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM (Linear)'],\n",
    "    'Accuracy': [log_reg_accuracy, rf_accuracy, svm_accuracy]\n",
    "}).sort_values(by='Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<30} {'Accuracy':>10}\")\n",
    "print(\"-\"*60)\n",
    "for idx, row in baseline_results.iterrows():\n",
    "    print(f\"{row['Model']:<30} {row['Accuracy']:>10.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select the best performing model\n",
    "best_model_row = baseline_results.iloc[0]\n",
    "best_model_name = best_model_row['Model']\n",
    "best_model_accuracy = best_model_row['Accuracy']\n",
    "\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    best_predictions = log_reg_predictions\n",
    "    best_model = log_reg_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_predictions = rf_predictions\n",
    "    best_model = rf_model\n",
    "elif best_model_name == 'SVM (Linear)':\n",
    "    best_predictions = svm_predictions\n",
    "    best_model = svm_model\n",
    "\n",
    "print(f\"\\n✓ Best Baseline Model: {best_model_name}\")\n",
    "print(f\"  Accuracy: {best_model_accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix for best model\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Get class names\n",
    "class_names = [label_encoder.inverse_transform([i])[0].replace('___', ' ') \n",
    "               for i in sorted(np.unique(y_test))]\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix: {best_model_name}', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(config.FIGURES_DIR / \"baseline_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Confusion matrix saved to: {config.FIGURES_DIR / 'baseline_confusion_matrix.png'}\")\n",
    "plt.show()\n",
    "\n",
    "# Save best model\n",
    "model_filename = f\"baseline_{best_model_name.lower().replace(' ', '_')}_model.pkl\"\n",
    "joblib.dump(best_model, config.MODELS_DIR / model_filename)\n",
    "print(f\"✓ Best model saved to: {config.MODELS_DIR / model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b2690-46f3-414a-a7ce-c352d9e2923e",
   "metadata": {},
   "source": [
    "## 5. Executive Summary: Baseline Results for FarmAI\n",
    "\n",
    "This notebook successfully established a performance benchmark for the **FarmAI Disease Detection System**.\n",
    "\n",
    "### Key Insights:\n",
    "1.  **Feature Power:** Using **EfficientNetB0** as a feature extractor proved highly effective. It transformed raw leaf pixels into meaningful patterns that even simple algorithms could understand.\n",
    "2.  **Baseline Accuracy:** The Classical Machine Learning models (Random Forest/SVM) achieved respectable accuracy scores. This confirms that the dataset is separable and the problem is solvable.\n",
    "3.  **Roadmap to Deep Learning:** While the baseline is strong, a custom-trained Deep Learning model (in the next notebook) is expected to outperform these results by **fine-tuning** the weights specifically for crop diseases, rather than just using pre-learned ImageNet features.\n",
    "\n",
    "**Next Step:** We will now proceed to build and train the final **FarmAI Deep Learning Pipeline (CNN)** for maximum accuracy and real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb6198-aa87-44e3-aac7-f095231127a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- BASELINE ANALYSIS COMPLETE ---\")\n",
    "print(\"Outputs generated:\")\n",
    "print(f\"  - Baseline Class Mapping: {config.MODELS_DIR / 'baseline_class_indices.json'}\")\n",
    "print(f\"  - Baseline Confusion Matrix Plot: {config.FIGURES_DIR / 'baseline_confusion_matrix.png'}\")\n",
    "print(f\"  - Best Baseline Model (e.g., Random Forest): {config.MODELS_DIR / 'baseline_random_forest_model.pkl'}\")\n",
    "print(\"\\nBaseline task finished successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f4d18-fe35-4485-b40e-f1c7dfc31c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (m1env)",
   "language": "python",
   "name": "m1env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
